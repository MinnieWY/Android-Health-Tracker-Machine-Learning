{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Data from Fitbit Web API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook used as a demonstarte with explanation on the code for retrieving raw data from the web api. It is mostly the same as the code in prediction except some of them are used for retrieving a period of data which us used as raw data for training model. Note that some of the features used in the model requires to acces the Intraday data (daily detail-level response). It is limited to Developer's account or a third-party application request is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions:\n",
    "1. Do not run all the code at the same time\n",
    "2. Replace the access token with the updated one (you can generate using Fitbit OAuth 2.0 Tutorial)\n",
    "3. For retrieving period of time data, do not set a long period of time (suggested not over 10 days)\n",
    "4. For date range input, Fitbit has a limit of 100 days per request. You may need to do merging by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Aceess_token (Generate using Fitbit OAuth 2.0 Tutorial)\n",
    "access_token = \"your_generate_access_token\"\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "start_date = datetime.datetime(2023, 11, 1)\n",
    "end_date = datetime.datetime(2024, 2, 15)\n",
    "\n",
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_date_str = end_date.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Sleep Log (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_log_data = []\n",
    "\n",
    "url = f\"https://api.fitbit.com/1.2/user/-/sleep/date/{start_date_str}/{end_date_str}.json\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "sleep_log_df = pd.DataFrame(data['sleep'])\n",
    "\n",
    "# Filter the type column to keep only rows where type is 0 (Sufficient data to generate log)\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['infoCode'] == 0]\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['isMainSleep'] == True]\n",
    "\n",
    "sleep_log_df.drop('logId', axis=1, inplace=True)\n",
    "sleep_log_df.drop('logType', axis=1, inplace=True)\n",
    "sleep_log_df.drop('infoCode', axis=1, inplace=True)\n",
    "sleep_log_df.drop('type', axis=1, inplace=True)\n",
    "sleep_log_df.drop('isMainSleep', axis=1, inplace=True)\n",
    "\n",
    "sleep_log_level = pd.DataFrame(sleep_log_df['levels'])\n",
    "new_df = sleep_log_df.loc[:, ['levels', 'dateOfSleep']]\n",
    "\n",
    "summary_data_list = []\n",
    "dates = []  # Store unique dateOfSleep values\n",
    "\n",
    "for index, row in new_df.iterrows():\n",
    "    string_df = row['levels']\n",
    "    summary_data = string_df['summary']\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    summary_data_list.append(df_summary)\n",
    "    dates.append(row['dateOfSleep'])\n",
    "\n",
    "# Create an empty list to store the modified DataFrames\n",
    "summary_data_modified = []\n",
    "\n",
    "# Iterate over the DataFrames in summary_data_list\n",
    "for df_summary, date in zip(summary_data_list, dates):\n",
    "    # Flatten the nested structure and rename the attributes\n",
    "    summary_data_flattened = {}\n",
    "    for element, attributes in df_summary.items():\n",
    "        for attribute, value in attributes.items():\n",
    "            if attribute != 'thirtyDayAvgMinutes':  # Exclude 'thirtyDayAvgMinutes' column\n",
    "                new_attribute = element + '_' + attribute\n",
    "                summary_data_flattened[new_attribute] = value\n",
    "    \n",
    "    # Create a new DataFrame with the modified data\n",
    "    df_summary_modified = pd.DataFrame(summary_data_flattened, index=[0])\n",
    "    df_summary_modified['dateOfSleep'] = date\n",
    "    \n",
    "    summary_data_modified.append(df_summary_modified)\n",
    "    \n",
    "# Concatenate the modified DataFrames into a single DataFrame\n",
    "summary_df = pd.concat(summary_data_modified, ignore_index=True)\n",
    "\n",
    "sleep_log_df.drop('levels', axis=1, inplace=True)\n",
    "\n",
    "merged_df = pd.merge(summary_df, sleep_log_df, on='dateOfSleep', how='left')\n",
    "\n",
    "\n",
    "merged_df['duration'] = merged_df['duration'] / (1000 * 60)\n",
    "\n",
    "merged_df['rem_proportion'] = merged_df['rem_minutes'] / merged_df['duration']\n",
    "merged_df['deep_proportion'] = merged_df['deep_minutes'] / merged_df['duration']\n",
    "merged_df['light_proportion'] = merged_df['light_minutes'] / merged_df['duration']\n",
    "merged_df['wake_proportion'] = merged_df['wake_minutes'] / merged_df['duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive sleep stage count\n",
    "sleep_log_df = pd.DataFrame(data['sleep'])\n",
    "\n",
    "# Filter the type column to keep only rows where type is 0 (Sufficient data to generate log)\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['infoCode'] == 0]\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['isMainSleep'] == True]\n",
    "\n",
    "sleep_log_df.drop('logId', axis=1, inplace=True)\n",
    "sleep_log_df.drop('logType', axis=1, inplace=True)\n",
    "sleep_log_df.drop('infoCode', axis=1, inplace=True)\n",
    "sleep_log_df.drop('type', axis=1, inplace=True)\n",
    "sleep_log_df.drop('isMainSleep', axis=1, inplace=True)\n",
    "\n",
    "sleep_log_level = pd.DataFrame(sleep_log_df['levels'])\n",
    "new_df = sleep_log_df.loc[:, ['levels', 'dateOfSleep']]\n",
    "\n",
    "summary_data_list = []\n",
    "dates = []  # Store unique dateOfSleep values\n",
    "new = pd.DataFrame()\n",
    "for index, row in new_df.iterrows():\n",
    "    string_df = row['levels']\n",
    "    summary_data = string_df['data']\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    df_summary['dateOfSleep'] = row['dateOfSleep']\n",
    "    new = pd.concat([new, df_summary], axis=0)\n",
    "\n",
    "# Create a pivot table to count the occurrences of each level for each day\n",
    "new_df = pd.pivot_table(new, index='dateOfSleep', columns='level', aggfunc='size', fill_value=0)\n",
    "\n",
    "new_df.to_csv('sleep_stage_count.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sleep patern detail\n",
    "sleep_log_df = pd.DataFrame(data['sleep'])\n",
    "\n",
    "# Filter the type column to keep only rows where type is 0 (Sufficient data to generate log)\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['infoCode'] == 0]\n",
    "sleep_log_df = sleep_log_df[sleep_log_df['isMainSleep'] == True]\n",
    "\n",
    "sleep_log_df.drop('logId', axis=1, inplace=True)\n",
    "sleep_log_df.drop('logType', axis=1, inplace=True)\n",
    "sleep_log_df.drop('infoCode', axis=1, inplace=True)\n",
    "sleep_log_df.drop('type', axis=1, inplace=True)\n",
    "sleep_log_df.drop('isMainSleep', axis=1, inplace=True)\n",
    "\n",
    "sleep_log_level = pd.DataFrame(sleep_log_df['levels'])\n",
    "new_df = sleep_log_df.loc[:, ['levels', 'dateOfSleep']]\n",
    "\n",
    "summary_data_list = []\n",
    "dates = []  # Store unique dateOfSleep values\n",
    "new = pd.DataFrame()\n",
    "for index, row in new_df.iterrows():\n",
    "    string_df = row['levels']\n",
    "    summary_data = string_df['data']\n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    df_summary['dateOfSleep'] = row['dateOfSleep']\n",
    "    new = pd.concat([new, df_summary], axis=0)\n",
    "\n",
    "new.to_csv('pattern2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Breathing Rate (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "breathing_rate_data = []\n",
    "\n",
    "date = start_date\n",
    "while date <= end_date:\n",
    "    date_str = date.strftime(\"%Y-%m-%d\")\n",
    "    url = f\"https://api.fitbit.com/1/user/-/br/date/{date_str}/all.json\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    data = response.json()\n",
    "\n",
    "    breathing_rate_data.extend(data['br'])\n",
    "\n",
    "    date += datetime.timedelta(days=1)\n",
    "\n",
    "breathing_rate_df = pd.DataFrame(breathing_rate_data)\n",
    "\n",
    "# Create new columns for each item in the 'value' column\n",
    "expanded_data = breathing_rate_df['value'].apply(pd.Series)\n",
    "\n",
    "# Concatenate the expanded columns with the original data\n",
    "breathing_data = pd.concat([breathing_rate_df['dateTime'], expanded_data], axis=1)\n",
    "\n",
    "breathing_data['deepSleepSummary'] = breathing_data['deepSleepSummary'].apply(lambda x: x['breathingRate'] if isinstance(x, dict) else x)\n",
    "breathing_data['remSleepSummary'] = breathing_data['remSleepSummary'].apply(lambda x: x['breathingRate'] if isinstance(x, dict) else x)\n",
    "breathing_data['fullSleepSummary'] = breathing_data['deepSleepSummary'].apply(lambda x: x['breathingRate'] if isinstance(x, dict) else x)\n",
    "breathing_data['lightSleepSummary'] = breathing_data['deepSleepSummary'].apply(lambda x: x['breathingRate'] if isinstance(x, dict) else x)\n",
    "\n",
    "breathing_data = breathing_data.rename(columns={'deepSleepSummary': 'deepSleep'})\n",
    "breathing_data = breathing_data.rename(columns={'remSleepSummary': 'remSleep'})\n",
    "breathing_data = breathing_data.rename(columns={'fullSleepSummary': 'fullSleep'})\n",
    "breathing_data = breathing_data.rename(columns={'lightSleepSummary': 'lightSleep'})\n",
    "\n",
    "breathing_data.to_csv('breathing.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resting Heart Rate (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = f\"https://api.fitbit.com/1.2/user/-/activities/heart/date/{start_date_str}/{end_date_str}.json\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "activites = pd.DataFrame(data['activities-heart'])\n",
    "\n",
    "new = pd.DataFrame()\n",
    "for index, row in activites.iterrows():\n",
    "    value_data = row['value']\n",
    "    if 'restingHeartRate' in value_data:\n",
    "        summary_data = value_data['restingHeartRate']\n",
    "    else:\n",
    "        summary_data = np.nan\n",
    "    summary_df = pd.DataFrame({'restingHeartRate': [summary_data]})\n",
    "    summary_df['dateTime'] = row['dateTime']\n",
    "    new = pd.concat([new, summary_df], axis=0)\n",
    "\n",
    "new.to_csv('resting_hr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Heart Rate Varability (By date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Aceess_token (Generate using Fitbit OAuth 2.0 Tutorial)\n",
    "access_token = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyM1I5SzQiLCJzdWIiOiJCUVpTRlkiLCJpc3MiOiJGaXRiaXQiLCJ0eXAiOiJhY2Nlc3NfdG9rZW4iLCJzY29wZXMiOiJyc29jIHJlY2cgcnNldCByb3h5IHJudXQgcnBybyByc2xlIHJjZiByYWN0IHJyZXMgcmxvYyByd2VpIHJociBydGVtIiwiZXhwIjoxNzA4NDYzMTYwLCJpYXQiOjE3MDg0MzQzNjB9.n5Hb-oYiStbXGQQJrwtUappQndmEBA8ilmaVPqeFNR4\"\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "date = datetime.datetime(2024, 2, 16).strftime(\"%Y-%m-%d\")\n",
    "url = f\"https://api.fitbit.com/1/user/-/hrv/date/{date}/all.json\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "dataset = data['hrv'][0]['minutes']\n",
    "hrv_df = pd.DataFrame(dataset)\n",
    "\n",
    "# # Extract values from the \"value\" column to new columns\n",
    "hrv_df = pd.concat([hrv_df.drop(['value'], axis=1), hrv_df['value'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(hrv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative Approach to merge all the data into single dataframe\n",
    "file_pattern = '../input/HRV/202402/*.csv'\n",
    "\n",
    "# Get a list of CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "concatenated_df = pd.concat(dfs)\n",
    "\n",
    "# Reset the index of the concatenated DataFrame\n",
    "concatenated_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Blood Oxgyen SPO2 (Period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach when using json file\n",
    "folder_path = '../input/spo2/'\n",
    "\n",
    "json_files = [f for f in os.listdir(folder_path) if f.endswith('.json')]\n",
    "\n",
    "for json_file in json_files:\n",
    "    date = json_file.split('.')[0]  # Extract the date from the file name\n",
    "    \n",
    "    # Read the JSON file\n",
    "    data = pd.read_json(os.path.join(folder_path, json_file), typ='series')\n",
    "    \n",
    "    # Extract the last part from the dataframe\n",
    "    minutes_data = data['minutes']\n",
    "    \n",
    "    # Create a DataFrame from the minutes data\n",
    "    df = pd.DataFrame(minutes_data)\n",
    "    df = df.rename(columns={'value': 'spo2'})\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    csv_file = f'spo2_{date}.csv'\n",
    "    df.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Blood Oxgyen SPO2 (By date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Aceess_token (Generate using Fitbit OAuth 2.0 Tutorial)\n",
    "access_token = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyM1I5SzQiLCJzdWIiOiJCUVpTRlkiLCJpc3MiOiJGaXRiaXQiLCJ0eXAiOiJhY2Nlc3NfdG9rZW4iLCJzY29wZXMiOiJyc29jIHJlY2cgcnNldCByb3h5IHJudXQgcnBybyByc2xlIHJjZiByYWN0IHJyZXMgcmxvYyByd2VpIHJociBydGVtIiwiZXhwIjoxNzA4NDYzMTYwLCJpYXQiOjE3MDg0MzQzNjB9.n5Hb-oYiStbXGQQJrwtUappQndmEBA8ilmaVPqeFNR4\"\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "date = datetime.datetime(2024, 2, 16).strftime(\"%Y-%m-%d\")\n",
    "url = f\"https://api.fitbit.com/1/user/-/spo2/date/{date}/all.json\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "dataset = data['minutes']\n",
    "spo2_df = pd.DataFrame(dataset)\n",
    "spo2_df.to_csv(f'spo2_detail_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrive Steps (By date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Aceess_token (Generate using Fitbit OAuth 2.0 Tutorial)\n",
    "access_token = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIyM1I5SzQiLCJzdWIiOiJCUVpTRlkiLCJpc3MiOiJGaXRiaXQiLCJ0eXAiOiJhY2Nlc3NfdG9rZW4iLCJzY29wZXMiOiJyc29jIHJlY2cgcnNldCByb3h5IHJudXQgcnBybyByc2xlIHJjZiByYWN0IHJyZXMgcmxvYyByd2VpIHJociBydGVtIiwiZXhwIjoxNzA4NDYzMTYwLCJpYXQiOjE3MDg0MzQzNjB9.n5Hb-oYiStbXGQQJrwtUappQndmEBA8ilmaVPqeFNR4\"\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "date = datetime.datetime(2024, 2, 16).strftime(\"%Y-%m-%d\")\n",
    "url = f\"https://api.fitbit.com/1/user/-/activities/date/{date}.json\"\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "steps = data['summary']['steps']\n",
    "df = pd.DataFrame({'date':date,'steps':steps }, index=[0])\n",
    "df.to_csv(f'steps_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging CSV into Single File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach to merge all the data into single dataframe using the CSV files\n",
    "file_pattern = './*.csv'  # Update with your file path pattern\n",
    "\n",
    "# Get a list of CSV files matching the pattern\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "    \n",
    "# Concatenate the DataFrames into a single DataFrame\n",
    "concatenated_df = pd.concat(dfs)\n",
    "\n",
    "concatenated_df.to_csv('attribute_name.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
